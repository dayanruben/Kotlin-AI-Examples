{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to Using Spring AI with Kotlin\n",
    "\n",
    "This notebook provides an introductory tutorial on using Spring AI in Kotlin to interact with large language models through an OpenAI example. We'll walk through the process step by step, covering configuration, using prompts, handling streaming responses, obtaining structured data, and utilizing tools.\n",
    "\n",
    "### Setting Up Your Project\n",
    "\n",
    "Ensure that your project includes the necessary Spring AI dependencies:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:34.054043Z",
     "start_time": "2025-03-12T17:17:33.628717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "USE {\n",
    "    dependencies {\n",
    "        val springAiVersion = \"1.0.0-M6\"\n",
    "        implementation(\"org.springframework.ai:spring-ai-openai:$springAiVersion\")\n",
    "        implementation(\"org.springframework.ai:spring-ai-openai-spring-boot-starter:$springAiVersion\")\n",
    "        implementation(\"com.fasterxml.jackson.module:jackson-module-kotlin:2.18.2\")\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Provide your OpenAI API key by setting up the `OPENAI_API_KEY` environmental variable. Alternatively, you can copy it here:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:34.122856Z",
     "start_time": "2025-03-12T17:17:34.060498Z"
    }
   },
   "cell_type": "code",
   "source": "val apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"YOUR_OPENAI_API_KEY\"",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set up the OpenAI chat model with your API key and configure the desired settings, such as temperature and model type:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:34.568877Z",
     "start_time": "2025-03-12T17:17:34.127784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.openai.OpenAiChatModel\n",
    "import org.springframework.ai.openai.OpenAiChatOptions\n",
    "import org.springframework.ai.openai.api.OpenAiApi\n",
    "\n",
    "val openAiApi = OpenAiApi(apiKey)\n",
    "val openAiChatOptions = OpenAiChatOptions.builder()\n",
    "    .model(OpenAiApi.ChatModel.GPT_4_O_MINI)\n",
    "    .temperature(0.7)\n",
    "    .build()\n",
    "val chatModel = OpenAiChatModel(openAiApi, openAiChatOptions)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sending Prompts\n",
    "\n",
    "Interact with the API by sending a prompt to the chat model and receiving a response:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:36.051753Z",
     "start_time": "2025-03-12T17:17:34.575222Z"
    }
   },
   "cell_type": "code",
   "source": "chatModel.call(\"Generate a hokku about Kotlin\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kotlin whispers soft,  \n",
       "In the realm of code it flows,  \n",
       "Concise, clear, and bright."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use Spring AI's `ChatClient` to create more complex prompts, such as providing system instructions:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:36.117162Z",
     "start_time": "2025-03-12T17:17:36.061236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.chat.client.ChatClient\n",
    "\n",
    "val chatClient = ChatClient.builder(chatModel).defaultSystem(\n",
    "    \"\"\"\n",
    "    You are a Lord of the Rings expert and a trusted advisor.\n",
    "    Offer wise, concise guidance in the style of Middle-earth,\n",
    "    drawing from its lore, characters, and philosophy.\n",
    "    \"\"\".trimIndent()\n",
    ").build()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now you can send a user-defined prompt to the chat model and retrieve the response content as a `String`:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:38.649546Z",
     "start_time": "2025-03-12T17:17:36.123172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt()\n",
    "    .user(\"What awaits us?\")\n",
    "    .call()\n",
    "    .content()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ah, dear seeker of wisdom, what awaits you is but a tapestry woven from the threads of choice, fate, and the will of the One. In the realms of Middle-earth, the path is often shrouded in shadow and light alike.\n",
       "\n",
       "Much like Frodo upon his journey to Mount Doom, you may find trials that test your resolve and courage. Yet remember, even the smallest person can change the course of the future. Seek the counsel of friends, for fellowship is a beacon in the darkest of times. \n",
       "\n",
       "Embrace the uncertainty, for therein lies the adventure. Heed the words of Gandalf, who spoke of the importance of the choices we make: \"All we have to decide is what to do with the time that is given us.\" Thus, prepare your heart for the journey ahead, and walk it with honor and hope."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Try replacing the `content()` call with `chatResponse()` to gain deeper insight into the response. `ChatResponse` represents the AI model's reply and includes metadata on how it was generated, such as the number of tokens used."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Handling Streaming Responses\n",
    "\n",
    "Using the `stream()` method, you receive partial chunks of the response as soon as they're ready. This approach allows you to avoid waiting for the AI to generate the entire response and enables you to display real-time progress to users."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Include the coroutine dependency to work with the result as a Kotlin `Flow`:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:39.119622Z",
     "start_time": "2025-03-12T17:17:38.658289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use coroutines\n",
    "@file:DependsOn(\"org.jetbrains.kotlinx:kotlinx-coroutines-reactive:1.10.1\")\n",
    "@file:DependsOn(\"org.jetbrains.kotlinx:kotlinx-coroutines-reactor:1.10.1\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In a reactive UI, you can show the incoming response in real time. To keep this example simple, we display each chunk of the response on a separate line (although they are printed simultaneously):"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:40.475314Z",
     "start_time": "2025-03-12T17:17:39.139252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kotlinx.coroutines.reactive.asFlow\n",
    "\n",
    "val streamingResponse: Flow<String> = chatModel\n",
    "    .stream(\"Generate a hokku about Kotlin\")\n",
    "    .asFlow()\n",
    "\n",
    "runBlocking {\n",
    "    streamingResponse.collect {\n",
    "        print(it)\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code flows like water,  \n",
      "Kotlin's breeze through the system,  \n",
      "Clean, concise, and bright."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Since `collect` is a suspend function, we wrap it inside a `runBlocking` call to use it within a notebook."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Structured Output\n",
    "\n",
    "Spring AI can automatically deserialize responses into Kotlin data classes, making it easy to handle structured outputs.\n",
    "\n",
    "Let's retrieve the response from the LLM about the movie in our desired format:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:40.544302Z",
     "start_time": "2025-03-12T17:17:40.480307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data class Movie(\n",
    "    val title: String,\n",
    "    val year: Int,\n",
    "    val director: String,\n",
    "    val genre: String\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Specify the `ResponseFormat` as `JSON_OBJECT` to instruct the LLM to return the output strictly in JSON, enabling Spring AI to automatically convert it into a `data` class:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:40.588300Z",
     "start_time": "2025-03-12T17:17:40.548573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.openai.api.ResponseFormat\n",
    "\n",
    "val structuredOutputOptions = OpenAiChatOptions.builder()\n",
    "    .model(OpenAiApi.ChatModel.GPT_4_O)\n",
    "    .responseFormat(ResponseFormat.builder().type(ResponseFormat.Type.JSON_OBJECT).build())\n",
    "    .build()\n",
    "val chatModelWithStructuredOutput = OpenAiChatModel(openAiApi, structuredOutputOptions)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the following example, OpenAI returns the requested JSON, which is automatically converted into a `Movie`:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:42.954760Z",
     "start_time": "2025-03-12T17:17:40.594308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ChatClient.create(chatModelWithStructuredOutput)\n",
    "    .prompt()\n",
    "    .user(\"Movie that won the Oscar for Best Picture in 1990\")\n",
    "    .call()\n",
    "    .entity(Movie::class.java)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Movie(title=Driving Miss Daisy, year=1990, director=Bruce Beresford, genre=Comedy-drama)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "AI models often hallucinate and aren't guaranteed to return correct answers. As a result, they may sometimes fail to produce the structured output as requested, instead returning something differentâ€”such as JSON with additional comments. Larger models tend to produce the expected output more consistently. In this example, selecting `GPT_4_O` rather than `GPT_4_O_MINI` yields both the correct movie choice ('Driving Miss Daisy') and properly formatted JSON. For real-life applications, consider implementing a validation mechanism to ensure the model's output matches the desired format."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using Tools"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tools allow LLMs to access your custom services in a powerful and flexible way. Let's use tools to work with OpenAI's function-calling feature and implement a weather service query.\n",
    "\n",
    "Without additional tools, the model won't provide information about the current weather, responding instead that it's unable to offer real-time weather updates:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:47.458208Z",
     "start_time": "2025-03-12T17:17:42.965912Z"
    }
   },
   "cell_type": "code",
   "source": "chatModel.call(\"What's the weather like in Paris today?\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I'm unable to provide real-time weather updates. To find out the current weather in Paris, I recommend checking a reliable weather website or app."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's imagine we have a weather service providing weather information for different locations. By using tools, we can give OpenAI access to this service. In this tutorial, we'll use `mockWeatherService` to simulate such a service:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:47.566212Z",
     "start_time": "2025-03-12T17:17:47.468992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fun mockWeatherService(location: String): Double? = when {\n",
    "    \"Paris\" in location -> 15.0\n",
    "    \"Tokyo\" in location -> 10.0\n",
    "    \"San Francisco\" in location -> 30.0\n",
    "    else -> null\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to grant the model access to the weather tool. First, we define a `FunctionTool` named `\"getCurrentWeather\"` with the description `\"Get the current temperature for a given location.\"` It includes one required property, `\"location\"`, of type `string`:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:47.604579Z",
     "start_time": "2025-03-12T17:17:47.570697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.model.ModelOptionsUtils\n",
    "\n",
    "val functionTool = OpenAiApi.FunctionTool(\n",
    "    OpenAiApi.FunctionTool.Type.FUNCTION,\n",
    "    OpenAiApi.FunctionTool.Function(\n",
    "        \"Get current temperature for a given location.\",\n",
    "        \"getCurrentWeather\", ModelOptionsUtils.jsonToMap(\n",
    "            \"\"\"\n",
    "                {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City and country e.g. BogotÃ¡, Colombia\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                    \"additionalProperties\": false\n",
    "                }\n",
    "                \"\"\".trimIndent()\n",
    "        ),\n",
    "        true\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we send the user's question along with the list of available tools."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:47.660431Z",
     "start_time": "2025-03-12T17:17:47.608978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.openai.api.OpenAiApi.*\n",
    "import org.springframework.ai.openai.api.OpenAiApi.ChatCompletionRequest.ToolChoiceBuilder\n",
    "\n",
    "val initialUserMessage = ChatCompletionMessage(\n",
    "    \"What's the weather like in Paris today?\",\n",
    "    ChatCompletionMessage.Role.USER\n",
    ")\n",
    "val chatCompletionRequest = ChatCompletionRequest(\n",
    "    listOf(initialUserMessage), \"gpt-4o\",\n",
    "    listOf(functionTool), ToolChoiceBuilder.AUTO\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Depending on the user's question, the model can now return a response containing information about the tools it chooses to use and the arguments required for those tools. If the user asks about the weather, the model selects our weather tool. If the user asks an unrelated question, the model behaves as usual. We can display the entire response to see which tools were chosen:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:49.071288Z",
     "start_time": "2025-03-12T17:17:47.665740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val chatCompletion = openAiApi.chatCompletionEntity(chatCompletionRequest)\n",
    "val responseFromLLM = chatCompletion.body!!.choices().first().message()\n",
    "responseFromLLM"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage[rawContent=null, role=ASSISTANT, name=null, toolCallId=null, toolCalls=[ToolCall[index=null, id=call_4FIXrL1H8Hv3fnNMChMIyf8A, type=function, function=ChatCompletionFunction[name=getCurrentWeather, arguments={\"location\":\"Paris, France\"}]]], refusal=null, audioOutput=null]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The response specifies the tool the LLM intends to call and its arguments:\n",
    "\n",
    "```function=ChatCompletionFunction[name=getCurrentWeather, arguments={\"location\":\"Paris, France\"}]```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We invoke the tool and send the result back to the model so that it can generate the final response for the userâ€”or possibly decide to call other tools based on the conversation."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:49.166122Z",
     "start_time": "2025-03-12T17:17:49.079234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lateinit var messageWithToolInvocation: ChatCompletionMessage\n",
    "for (toolCall in responseFromLLM.toolCalls()) {\n",
    "    when (val functionName = toolCall.function().name()) {\n",
    "        \"getCurrentWeather\" -> {\n",
    "            val location = toolCall.function().arguments()\n",
    "            val temperature = mockWeatherService(location)\n",
    "            messageWithToolInvocation = ChatCompletionMessage(\n",
    "                if (temperature != null) \"$temperature C\" else \"Unable to get the weather\",\n",
    "                ChatCompletionMessage.Role.TOOL,\n",
    "                functionName, toolCall.id(), null, null, null\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we send all the messages to the LLM to provide the full context: the initial message, the response with the tool choice, and the tool invocation result. With this information, the LLM can now answer the user's initial question about the current weather in Paris:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:17:50.205188Z",
     "start_time": "2025-03-12T17:17:49.171097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val messages = mutableListOf(initialUserMessage, responseFromLLM, messageWithToolInvocation)\n",
    "val functionResponseRequest = ChatCompletionRequest(messages, \"gpt-4o\", 0.2)\n",
    "val resultingCompletion = openAiApi.chatCompletionEntity(functionResponseRequest)\n",
    "resultingCompletion.body!!.choices().first().message().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The current temperature in Paris, France is 15Â°C."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The LLM successfully used the provided tool to respond to the user. Enhancing LLMs with external tools can automate tasks such as data retrieval, customer support, and IoT control."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook serves as an overview of how to integrate Spring AI into your Kotlin projects, enabling you to build powerful AI-driven applications. Experiment further with prompts and tailored implementations for your specific needs! ðŸš€"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  },
  "ktnbPluginMetadata": {
   "projectLibraries": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
