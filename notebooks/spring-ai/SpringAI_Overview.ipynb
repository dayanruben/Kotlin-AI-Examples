{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to Using Spring AI with Kotlin\n",
    "\n",
    "This notebook provides an introductory tutorial on using Spring AI in Kotlin to interact with large language models through an OpenAI example. We'll walk through the process step by step, covering configuration, using prompts, handling streaming responses, obtaining structured data, and utilizing tools.\n",
    "\n",
    "### Setting Up Your Project\n",
    "\n",
    "Ensure that your project includes the necessary Spring AI dependencies:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:46.453940Z",
     "start_time": "2025-05-11T18:51:45.261870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use spring-ai-openai\n",
    "USE { dependencies { implementation(\"com.fasterxml.jackson.module:jackson-module-kotlin:2.18.2\") } }"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Provide your OpenAI API key by setting up the `OPENAI_API_KEY` environmental variable. Alternatively, you can copy it here:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:46.513875Z",
     "start_time": "2025-05-11T18:51:46.461778Z"
    }
   },
   "cell_type": "code",
   "source": "val apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"YOUR_OPENAI_API_KEY\"",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set up the OpenAI chat model with your API key and configure the desired settings, such as temperature and model type:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:46.774329Z",
     "start_time": "2025-05-11T18:51:46.518004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val openAiApi = OpenAiApi.builder().apiKey(apiKey).build()\n",
    "val openAiChatOptions = OpenAiChatOptions.builder()\n",
    "    .model(OpenAiApi.ChatModel.GPT_4_O_MINI)\n",
    "    .temperature(0.7)\n",
    "    .build()\n",
    "val chatModel = OpenAiChatModel.builder().openAiApi(openAiApi).defaultOptions(openAiChatOptions).build()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sending Prompts\n",
    "\n",
    "Interact with the API by sending a prompt to the chat model and receiving a response:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:47.820009Z",
     "start_time": "2025-05-11T18:51:46.791193Z"
    }
   },
   "cell_type": "code",
   "source": "chatModel.call(\"Generate a hokku about Kotlin\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In code's embrace,  \n",
       "Kotlin whispers, smooth and clearâ€”  \n",
       "Nature meets the tech."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use Spring AI's `ChatClient` to create more complex prompts, such as providing system instructions:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:47.874900Z",
     "start_time": "2025-05-11T18:51:47.823943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val chatClient = ChatClient.builder(chatModel).defaultSystem(\n",
    "    \"\"\"\n",
    "    You are a Lord of the Rings expert and a trusted advisor.\n",
    "    Offer wise, concise guidance in the style of Middle-earth,\n",
    "    drawing from its lore, characters, and philosophy.\n",
    "    \"\"\".trimIndent()\n",
    ").build()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now you can send a user-defined prompt to the chat model and retrieve the response content as a `String`:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:51.046247Z",
     "start_time": "2025-05-11T18:51:47.878935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt()\n",
    "    .user(\"What awaits us?\")\n",
    "    .call()\n",
    "    .content()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ah, dear traveler, the road ahead is ever uncertain, much like the paths through the Shire or the shadowed trails of Mirkwood. What awaits you may be shaped by the choices you make, for in Middle-earth, destiny often intertwines with the will of the Free Peoples.\n",
       "\n",
       "Take heed of the wisdom of Gandalf, who reminds us that \"All we have to decide is what to do with the time that is given us.\" Embrace courage in the face of adversity, for even the smallest person can change the course of the future, as Frodo Baggins so bravely did.\n",
       "\n",
       "Seek fellowship in your journey, for the bonds of friendship and loyalty are as vital as the strongest steel. And remember, even in the darkest of times, the light of hope can shine through, much like the Evenstar guiding those lost in shadow.\n",
       "\n",
       "Prepare thyself, for every adventure holds both peril and wonder. Trust in your heart, and let the tales of old inspire you as you forge your own path."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Try replacing the `content()` call with `chatResponse()` to gain deeper insight into the response. `ChatResponse` represents the AI model's reply and includes metadata on how it was generated, such as the number of tokens used."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Handling Streaming Responses\n",
    "\n",
    "Using the `stream()` method, you receive partial chunks of the response as soon as they're ready. This approach allows you to avoid waiting for the AI to generate the entire response and enables you to display real-time progress to users."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Include the coroutine dependency to work with the result as a Kotlin `Flow`:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:51.135360Z",
     "start_time": "2025-05-11T18:51:51.053181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use coroutines\n",
    "@file:DependsOn(\"org.jetbrains.kotlinx:kotlinx-coroutines-reactive:1.10.1\")\n",
    "@file:DependsOn(\"org.jetbrains.kotlinx:kotlinx-coroutines-reactor:1.10.1\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In a reactive UI, you can show the incoming response in real time. To keep this example simple, we display each chunk of the response on a separate line (although they are printed simultaneously):"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:52.236596Z",
     "start_time": "2025-05-11T18:51:51.138698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kotlinx.coroutines.reactive.asFlow\n",
    "\n",
    "val streamingResponse: Flow<String> = chatModel\n",
    "    .stream(\"Generate a hokku about Kotlin\")\n",
    "    .asFlow()\n",
    "\n",
    "runBlocking {\n",
    "    streamingResponse.collect {\n",
    "        print(it)\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In code's gentle flow,  \n",
      "Kotlin whispers, clean and brightâ€”  \n",
      "Breezes of pure joy."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Since `collect` is a suspend function, we wrap it inside a `runBlocking` call to use it within a notebook."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Structured Output\n",
    "\n",
    "Spring AI can automatically deserialize responses into Kotlin data classes, making it easy to handle structured outputs.\n",
    "\n",
    "Let's retrieve the response from the LLM about the movie in our desired format:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:52.300410Z",
     "start_time": "2025-05-11T18:51:52.240029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data class Movie(\n",
    "    val title: String,\n",
    "    val year: Int,\n",
    "    val director: String,\n",
    "    val genre: String\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Specify the `ResponseFormat` as `JSON_OBJECT` to instruct the LLM to return the output strictly in JSON, enabling Spring AI to automatically convert it into a `data` class:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:52.337347Z",
     "start_time": "2025-05-11T18:51:52.302905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val structuredOutputOptions = OpenAiChatOptions.builder()\n",
    "    .model(OpenAiApi.ChatModel.GPT_4_O_MINI)\n",
    "    .responseFormat(ResponseFormat.builder().type(ResponseFormat.Type.JSON_OBJECT).build())\n",
    "    .build()\n",
    "val chatModelWithStructuredOutput = OpenAiChatModel.builder().openAiApi(openAiApi).defaultOptions(structuredOutputOptions).build()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the following example, OpenAI returns the requested JSON, which is automatically converted into a `Movie`:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:53.419457Z",
     "start_time": "2025-05-11T18:51:52.342096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ChatClient.create(chatModelWithStructuredOutput)\n",
    "    .prompt()\n",
    "    .user(\"Movie that won the Oscar for Best Picture in 1990\")\n",
    "    .call()\n",
    "    .entity<Movie>()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Movie(title=Driving Miss Daisy, year=1989, director=Driving Miss Daisy, genre=Drama)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "AI models often hallucinate and aren't guaranteed to return correct answers. As a result, they may sometimes fail to produce the structured output as requested, instead returning something differentâ€”such as JSON with additional comments. Larger models tend to produce the expected output more consistently. In this example, selecting `GPT_4_O` rather than `GPT_4_O_MINI` yields both the correct movie choice ('Driving Miss Daisy') and properly formatted JSON. For real-life applications, consider implementing a validation mechanism to ensure the model's output matches the desired format."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using Tools"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tools allow LLMs to access your custom services in a powerful and flexible way. Let's use tools to work with OpenAI's function-calling feature and implement a weather service query.\n",
    "\n",
    "Without additional tools, the model won't provide information about the current weather, responding instead that it's unable to offer real-time weather updates:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:54.397916Z",
     "start_time": "2025-05-11T18:51:53.425168Z"
    }
   },
   "cell_type": "code",
   "source": "chatModel.call(\"What's the weather like in Paris today?\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I'm unable to provide real-time weather updates. I recommend checking a reliable weather website or app for the most current conditions in Paris."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's imagine we have a weather service providing weather information for different locations. By using tools, we can give OpenAI access to this service. In this tutorial, we'll use `mockWeatherService` to simulate such a service:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:51:54.475702Z",
     "start_time": "2025-05-11T18:51:54.402514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fun mockWeatherService(location: String): Double? = when {\n",
    "    \"Paris\" in location -> 15.0\n",
    "    \"Tokyo\" in location -> 10.0\n",
    "    \"San Francisco\" in location -> 30.0\n",
    "    else -> null\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to grant the model access to the weather tool. First, we define a `FunctionTool` named `\"getCurrentWeather\"` with the description `\"Get the current temperature for a given location.\"` It includes one required property, `\"location\"`, of type `string`:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:52:11.082672Z",
     "start_time": "2025-05-11T18:52:11.044812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.model.ModelOptionsUtils\n",
    "\n",
    "val functionTool = OpenAiApi.FunctionTool(\n",
    "    OpenAiApi.FunctionTool.Type.FUNCTION,\n",
    "    OpenAiApi.FunctionTool.Function(\n",
    "        \"Get current temperature for a given location.\",\n",
    "        \"getCurrentWeather\", ModelOptionsUtils.jsonToMap(\n",
    "            \"\"\"\n",
    "                {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City and country e.g. BogotÃ¡, Colombia\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                    \"additionalProperties\": false\n",
    "                }\n",
    "                \"\"\".trimIndent()\n",
    "        ),\n",
    "        true\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we send the user's question along with the list of available tools."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:53:21.055393Z",
     "start_time": "2025-05-11T18:53:21.007170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.openai.api.OpenAiApi.ChatCompletionMessage\n",
    "import org.springframework.ai.openai.api.OpenAiApi.ChatCompletionRequest\n",
    "import org.springframework.ai.openai.api.OpenAiApi.ChatCompletionRequest.ToolChoiceBuilder\n",
    "\n",
    "val initialUserMessage = ChatCompletionMessage(\n",
    "    \"What's the weather like in Paris today?\",\n",
    "    ChatCompletionMessage.Role.USER\n",
    ")\n",
    "val chatCompletionRequest = ChatCompletionRequest(\n",
    "    listOf(initialUserMessage), \"gpt-4o\",\n",
    "    listOf(functionTool), ToolChoiceBuilder.AUTO\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Depending on the user's question, the model can now return a response containing information about the tools it chooses to use and the arguments required for those tools. If the user asks about the weather, the model selects our weather tool. If the user asks an unrelated question, the model behaves as usual. We can display the entire response to see which tools were chosen:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:53:23.993702Z",
     "start_time": "2025-05-11T18:53:22.992199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val chatCompletion = openAiApi.chatCompletionEntity(chatCompletionRequest)\n",
    "val responseFromLLM = chatCompletion.body!!.choices().first().message()\n",
    "responseFromLLM"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage[rawContent=null, role=ASSISTANT, name=null, toolCallId=null, toolCalls=[ToolCall[index=null, id=call_j7bF9U678spoE9ZFgLydbS7Y, type=function, function=ChatCompletionFunction[name=getCurrentWeather, arguments={\"location\":\"Paris, France\"}]]], refusal=null, audioOutput=null]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The response specifies the tool the LLM intends to call and its arguments:\n",
    "\n",
    "```function=ChatCompletionFunction[name=getCurrentWeather, arguments={\"location\":\"Paris, France\"}]```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We invoke the tool and send the result back to the model so that it can generate the final response for the userâ€”or possibly decide to call other tools based on the conversation."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:53:26.592539Z",
     "start_time": "2025-05-11T18:53:26.518174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lateinit var messageWithToolInvocation: ChatCompletionMessage\n",
    "for (toolCall in responseFromLLM.toolCalls()) {\n",
    "    when (val functionName = toolCall.function().name()) {\n",
    "        \"getCurrentWeather\" -> {\n",
    "            val location = toolCall.function().arguments()\n",
    "            val temperature = mockWeatherService(location)\n",
    "            messageWithToolInvocation = ChatCompletionMessage(\n",
    "                if (temperature != null) \"$temperature C\" else \"Unable to get the weather\",\n",
    "                ChatCompletionMessage.Role.TOOL,\n",
    "                functionName, toolCall.id(), null, null, null\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we send all the messages to the LLM to provide the full context: the initial message, the response with the tool choice, and the tool invocation result. With this information, the LLM can now answer the user's initial question about the current weather in Paris:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:53:29.327398Z",
     "start_time": "2025-05-11T18:53:28.690337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val messages = mutableListOf(initialUserMessage, responseFromLLM, messageWithToolInvocation)\n",
    "val functionResponseRequest = ChatCompletionRequest(messages, \"gpt-4o\", 0.2)\n",
    "val resultingCompletion = openAiApi.chatCompletionEntity(functionResponseRequest)\n",
    "resultingCompletion.body!!.choices().first().message().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The current temperature in Paris, France is 15Â°C."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The LLM successfully used the provided tool to respond to the user. Enhancing LLMs with external tools can automate tasks such as data retrieval, customer support, and IoT control."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook serves as an overview of how to integrate Spring AI into your Kotlin projects, enabling you to build powerful AI-driven applications. Experiment further with prompts and tailored implementations for your specific needs! ðŸš€"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  },
  "ktnbPluginMetadata": {
   "projectLibraries": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
