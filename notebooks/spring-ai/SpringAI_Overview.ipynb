{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to Using Spring AI with Kotlin\n",
    "\n",
    "This notebook provides an introductory tutorial on using Spring AI in Kotlin to interact with large language models, using an OpenAI example. We'll go through the process step by step to configure it, use prompts, handle streaming responses, obtain structured data as a result and use tools.\n",
    "\n",
    "### Setting Up Your Project\n",
    "\n",
    "Make sure your project is set up to include the Spring AI dependencies:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "USE {\n",
    "    repositories {\n",
    "        mavenCentral()\n",
    "        maven(url = \"https://repo.spring.io/milestone\")\n",
    "        maven(url = \"https://repo.spring.io/snapshot\")\n",
    "    }\n",
    "\n",
    "    dependencies {\n",
    "        val springAiVersion = \"1.0.0-M5\"\n",
    "        implementation(\"org.springframework.ai:spring-ai-openai:$springAiVersion\")\n",
    "        implementation(\"org.springframework.ai:spring-ai-openai-spring-boot-starter:$springAiVersion\")\n",
    "        implementation(\"com.fasterxml.jackson.module:jackson-module-kotlin:2.18.2\")\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Provide your OpenAI API key by setting up an environmental variable `OPENAI_API_KEY`. Alternatively, you can copy it here:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "val apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"YOUR_OPENAI_API_KEY\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set up the OpenAI chat model with your API key and specify the desired configuration (such as temperature and model type):"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.openai.OpenAiChatModel\n",
    "import org.springframework.ai.openai.OpenAiChatOptions\n",
    "import org.springframework.ai.openai.api.OpenAiApi\n",
    "\n",
    "val openAiApi = OpenAiApi(apiKey)\n",
    "val openAiChatOptions = OpenAiChatOptions.builder()\n",
    "    .model(OpenAiApi.ChatModel.GPT_4_O_MINI)\n",
    "    .temperature(0.3)\n",
    "    .build()\n",
    "val chatModel = OpenAiChatModel(openAiApi, openAiChatOptions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sending Prompts\n",
    "\n",
    "Interact with the API by sending a simple prompt to the chat model and receiving a response:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chatModel.call(\"Generate a hokku about Kotlin\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use Spring AI's `ChatClient` to define more complex prompts, such as providing system instructions:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.chat.client.ChatClient\n",
    "\n",
    "val chatClient = ChatClient.builder(chatModel).defaultSystem(\n",
    "    \"\"\"\n",
    "    You are a Lord of the Rings expert and a trusted advisor.\n",
    "    Offer wise, concise guidance in the style of Middle-earth,\n",
    "    drawing from its lore, characters, and philosophy.\n",
    "    \"\"\".trimIndent()\n",
    ").build()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now you can send a user-defined prompt to a chat model and retrieve the response content as `String`:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt()\n",
    "    .user(\"What awaits us?\")\n",
    "    .call()\n",
    "    .content()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Try replacing the `content()` call with `chatResponse()` to gain more insight into the response. `ChatResponse` represents the AI model's response and includes metadata on how it was generated, such as the number of tokens used."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Handling Streaming Responses\n",
    "\n",
    "Using the `stream()` method you get unfinished chunks of response when they are ready. This way, you don't wait for the AI to generate an entire response, and can show real-time progress to users.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Include the dependency on coroutines to work with the result as a Kotlin `Flow`:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use coroutines\n",
    "@file:DependsOn(\"org.jetbrains.kotlinx:kotlinx-coroutines-reactive:1.10.1\")\n",
    "@file:DependsOn(\"org.jetbrains.kotlinx:kotlinx-coroutines-reactor:1.10.1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In a reactive UI, you can show the incoming response in real time. To keep this example simple, we display different chunks of the response, each on a separate line (though they are printed simultaneously):"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import kotlinx.coroutines.reactive.asFlow\n",
    "\n",
    "val streamingResponse: Flow<String> = chatModel\n",
    "    .stream(\"Generate a hokku about Kotlin\")\n",
    "    .asFlow()\n",
    "\n",
    "runBlocking {\n",
    "    streamingResponse.collect {\n",
    "        println(it)\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since `collect` is a suspend function, we surround it with the `runBlocking` call to use it in a notebook."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Structured Output\n",
    "\n",
    "Spring AI allows you to deserialize the responses into Kotlin data classes automatically, making it easy to handle structured outputs.\n",
    "\n",
    "Let's get the response from LLM about the movie in the desired format:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data class Movie(\n",
    "    val title: String,\n",
    "    val year: Int,\n",
    "    val director: String,\n",
    "    val genre: String\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Specify `ResponseFormat` as `JSON_OBJECT` to instruct LLM to return the output strictly in JSON and Spring AI to automatically convert it to a `data` class:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.openai.api.ResponseFormat\n",
    "\n",
    "val structuredOutputOptions = OpenAiChatOptions.builder()\n",
    "    .model(OpenAiApi.ChatModel.GPT_4_O)\n",
    "    .responseFormat(ResponseFormat.builder().type(ResponseFormat.Type.JSON_OBJECT).build())\n",
    "    .build()\n",
    "val chatModelWithStructuredOutput = OpenAiChatModel(openAiApi, structuredOutputOptions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the following example, OpenAI returns the required JSON, which gets automatically converted to a `Movie`:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ChatClient.create(chatModelWithStructuredOutput)\n",
    "    .prompt()\n",
    "    .user(\"Movie that won the Oscar for Best Picture in 1990\")\n",
    "    .call()\n",
    "    .entity(Movie::class.java)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As AI models often hallucinate and are not guaranteed to return the correct answer, they may sometimes fail to produce the structured output as requested, instead returning something else, such as JSON with additional comments. The larger the model, the more consistently it returns the expected output. In this example, choosing `GPT_4_O` over `GPT_4_O_MINI` results in the correct movie choice ('Driving Miss Daisy') as well as properly formatted JSON. In real-life applications, consider implementing a validation mechanism to ensure the model's output is in the correct format."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using Tools"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tools allow LLMs  access your custom services in a powerful and flexible way. Let's use tools to work with OpenAI's function calling, and implement a weather service query.\n",
    "\n",
    "Without additional tools, the model won't provide any information about the current weather, saying that it's unable to provide real-time weather updates:\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chatModel.call(\"What's the weather like in Paris today?\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's imagine we have a weather service providing weather information for different locations. Using tools, we can equip OpenAI with the access to this service. In this tutorial, we'll use `mockWeatherService` to mock such a service:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fun mockWeatherService(location: String): Double? = when {\n",
    "    \"Paris\" in location -> 15.0\n",
    "    \"Tokyo\" in location -> 10.0\n",
    "    \"San Francisco\" in location -> 30.0\n",
    "    else -> null\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to provide the model access to the weather tool. First, we define a `FunctionTool` with the name `\"getCurrentWeather\"` and the description `\"Get current temperature for a given location.\"`:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.model.ModelOptionsUtils\n",
    "\n",
    "val functionTool = FunctionTool(\n",
    "    FunctionTool.Type.FUNCTION,\n",
    "    FunctionTool.Function(\n",
    "        \"Get current temperature for a given location.\",\n",
    "        \"getCurrentWeather\", ModelOptionsUtils.jsonToMap(\n",
    "            \"\"\"\n",
    "                {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City and country e.g. BogotÃ¡, Colombia\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                    \"additionalProperties\": false\n",
    "                }\n",
    "                \"\"\".trimIndent()\n",
    "        ),\n",
    "        true\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we send the user question together with the list of available tools:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.openai.api.OpenAiApi.*\n",
    "import org.springframework.ai.openai.api.OpenAiApi.ChatCompletionRequest.ToolChoiceBuilder\n",
    "\n",
    "val initialUserMessage = ChatCompletionMessage(\n",
    "    \"What's the weather like in Paris today?\",\n",
    "    ChatCompletionMessage.Role.USER\n",
    ")\n",
    "val chatCompletionRequest = ChatCompletionRequest(\n",
    "    listOf(initialUserMessage), \"gpt-4o\",\n",
    "    listOf(functionTool), ToolChoiceBuilder.AUTO\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Depending on the user question, the model can now return the response with the information about the tools it decides to use and the arguments to provide to these tools. If the user asks about the weather, the model decides to use our weather tool. If the user asks an unrelated question, the model behaves as usual. We can display the whole response to check the chosen tools:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val chatCompletion = openAiApi.chatCompletionEntity(chatCompletionRequest)\n",
    "val responseFromLLM = chatCompletion.body!!.choices().first().message()\n",
    "responseFromLLM"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The response specifies the tool the LLM wants to call and its arguments:\n",
    "\n",
    "```function=ChatCompletionFunction[name=getCurrentWeather, arguments={\"location\":\"Paris, France\"}]```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We invoke the tool and send the result back to the model, so that it could prepare the final response to the user (or, probably, decide to call other tools depending on the conversation):"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lateinit var messageWithToolInvocation: ChatCompletionMessage\n",
    "for (toolCall in responseFromLLM.toolCalls()) {\n",
    "    when (val functionName = toolCall.function().name()) {\n",
    "        \"getCurrentWeather\" -> {\n",
    "            val location = toolCall.function().arguments()\n",
    "            val temperature = mockWeatherService(location)\n",
    "            messageWithToolInvocation = ChatCompletionMessage(\n",
    "                if (temperature != null) \"$temperature C\" else \"Unable to get the weather\",\n",
    "                ChatCompletionMessage.Role.TOOL,\n",
    "                functionName, toolCall.id(), null, null, null\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we send all the messages to LLM to provide the full context: the initial message, the response with the tool choice and the tool invocation result. Given this information, LLM can now answer the initial user question about the current weather in Paris:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val messages = mutableListOf(initialUserMessage, responseFromLLM, messageWithToolInvocation)\n",
    "val functionResponseRequest = ChatCompletionRequest(messages, \"gpt-4o\", 0.2)\n",
    "val resultingCompletion = openAiApi.chatCompletionEntity(functionResponseRequest)\n",
    "resultingCompletion.body!!.choices().first().message().content()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LLM was able to use the provided tool to respond to the user. Enhancing LLMs with external tools can automate tasks like data retrieval, customer support, and IoT control."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook serves as an overview of how to integrate Spring AI into your Kotlin projects, empowering you to create powerful AI-driven applications. Experiment further with prompts and tailored implementations for your specific needs! ðŸš€"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  },
  "ktnbPluginMetadata": {
   "projectLibraries": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
